{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Towards Reducing Pollution in Kigali\n### Predict Vehicles' consumption in liters/100km for each car in the city","metadata":{}},{"cell_type":"markdown","source":"Introduction\n\nPollution is a big issue in the city of Kigali. Policy makers in the city want to take action and deploy some measures to address this problem. You have been hired as a machine learning expert to analyze some data and help them make good decisions.\n\nCars that consume more fuel pollute more. As a first step, we want to estimate how much fuel each individual car consumes every 100 km. The provided dataset concerns city-cycle fuel consumption in liters per 100 kilometers (target).\n\nThe aim of this homework is to help you apply the skills that you have learned so far to a real dataset. This involves learning what data means, how to handle and visualize data, training, cross validation, prediction, testing your model, etc.\n\nDescription of covariates\nThis dataset has 3 multi-valued discrete and 5 continuous covariates\n\n\n    1. cylinders:     multi-valued discrete\n    2. displacement:  continuous\n    3. horsepower:    continuous\n    4. weight:        continuous\n    5. acceleration:  continuous\n    6. model year:    multi-valued discrete\n    7. origin:        multi-valued discrete\n    8. car name:      string (unique for each instance)","metadata":{}},{"cell_type":"code","source":"# as usual, let us load all the necessary libraries\nimport numpy as np  # numerical computation with arrays\nimport pandas as pd # library to manipulate datasets using dataframes\nimport scipy as sp  # statistical library\n\n\n# below sklearn libraries for different models\nfrom sklearn.tree import DecisionTreeClassifier as DecisionTree\nfrom sklearn.ensemble import RandomForestClassifier as RandomForest\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\n\n#import libraries for implementing neural networks\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD, Adam\nfrom keras.regularizers import l2\n\n# plot \nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"id":"spnSb2MwuGWA","execution":{"iopub.status.busy":"2021-08-13T12:51:24.5665Z","iopub.execute_input":"2021-08-13T12:51:24.566847Z","iopub.status.idle":"2021-08-13T12:51:24.577042Z","shell.execute_reply.started":"2021-08-13T12:51:24.566818Z","shell.execute_reply":"2021-08-13T12:51:24.576331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Additional Libraries\n\nimport seaborn as sns\nfrom scipy.stats import zscore\nfrom sklearn.svm import SVR\nfrom sklearn.svm import LinearSVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"j75caU1TnUcS","execution":{"iopub.status.busy":"2021-08-13T12:51:24.578246Z","iopub.execute_input":"2021-08-13T12:51:24.57868Z","iopub.status.idle":"2021-08-13T12:51:24.593924Z","shell.execute_reply.started":"2021-08-13T12:51:24.578652Z","shell.execute_reply":"2021-08-13T12:51:24.593124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Competition Data\nRun all the steps below to obtain the data","metadata":{"id":"bANkoD_UWdZM"}},{"cell_type":"code","source":"# Read training data\ntrain_data = pd.read_csv('https://raw.githubusercontent.com/onefishy/Rwanda-course-2020/master/Competition_data/train.csv',na_values='?') # read in the data as a DataFrame\ntrain_data.head(3) # show the first 3 rows of the dataset","metadata":{"id":"emBK0x1i8Vf1","outputId":"c3f93ec7-b592-4562-9cac-8712001ab5b9","execution":{"iopub.status.busy":"2021-08-13T12:51:24.595301Z","iopub.execute_input":"2021-08-13T12:51:24.595573Z","iopub.status.idle":"2021-08-13T12:51:24.697921Z","shell.execute_reply.started":"2021-08-13T12:51:24.595547Z","shell.execute_reply":"2021-08-13T12:51:24.697061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read test data \ntest_data = pd.read_csv('https://raw.githubusercontent.com/onefishy/Rwanda-course-2020/master/Competition_data/test.csv') # read in the data as a DataFrame\ntest_data.head(3) # show the first 3 rows of the dataset","metadata":{"id":"UqrhOJv_Lafq","outputId":"be3e68dc-2b05-4f52-dc06-be91cc1be779","execution":{"iopub.status.busy":"2021-08-13T12:51:24.6993Z","iopub.execute_input":"2021-08-13T12:51:24.699549Z","iopub.status.idle":"2021-08-13T12:51:24.79449Z","shell.execute_reply.started":"2021-08-13T12:51:24.699525Z","shell.execute_reply":"2021-08-13T12:51:24.793548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now we are done with downloading data! \n* Try building a model inside this notebook by create additional cells below with code to specify and fit the model\n* If you are fitting large neural nets, make sure this google colab notebook is running on GPUs\n* Check Edit --> Notebook settings --> Hardware accelerator: GPU","metadata":{"id":"WDifU46dYBEB"}},{"cell_type":"code","source":"# Start building a model here","metadata":{"id":"ZcDDw0AbWTKX","execution":{"iopub.status.busy":"2021-08-13T12:51:24.795902Z","iopub.execute_input":"2021-08-13T12:51:24.796291Z","iopub.status.idle":"2021-08-13T12:51:24.800965Z","shell.execute_reply.started":"2021-08-13T12:51:24.79625Z","shell.execute_reply":"2021-08-13T12:51:24.80011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{"id":"6le8uennL9Dc"}},{"cell_type":"code","source":"train_data['weight']=train_data['weight'].abs()#Change negative values in the weight column\ntrain_data['horsepower'] = pd.to_numeric(train_data['horsepower'], errors='coerce')#Chnage the horsepower column to numerical from object\ntrain_data=train_data.drop(['car name'],axis=1)#Drop the car name column in train data since we don't need it for prediction due to its Uniqueness\ntest_data=test_data.drop(['car name'],axis=1)#Drop the car name column in test data since we don't need it for prediction due to its Uniqueness","metadata":{"id":"DybcQTgLxnZx","execution":{"iopub.status.busy":"2021-08-13T12:51:24.80279Z","iopub.execute_input":"2021-08-13T12:51:24.803078Z","iopub.status.idle":"2021-08-13T12:51:24.814064Z","shell.execute_reply.started":"2021-08-13T12:51:24.803052Z","shell.execute_reply":"2021-08-13T12:51:24.813205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check for Null values","metadata":{"id":"ahEGjtQpJEgd"}},{"cell_type":"code","source":"train_data.isna().sum() #check for null values in the dataset","metadata":{"id":"sEmwBuvPI7Go","outputId":"809653c3-88ee-436b-855c-231a21f053be","execution":{"iopub.status.busy":"2021-08-13T12:51:24.815632Z","iopub.execute_input":"2021-08-13T12:51:24.81592Z","iopub.status.idle":"2021-08-13T12:51:24.830914Z","shell.execute_reply.started":"2021-08-13T12:51:24.815882Z","shell.execute_reply":"2021-08-13T12:51:24.830307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imputing Null values with Liner Regression Model\n Since our data has liner pattern we will apply liner regression imputer to fill the null values by learning the non-null datapoints","metadata":{"id":"GkuL0cBg3ORS"}},{"cell_type":"code","source":"#Select column with null values\nnull_data = train_data[train_data.isnull().any(axis=1)]#check for null values in the training set and assign to local variable\ntrain_data2 = train_data.dropna()#select all non-null columns by dropping the null values\n\nx_imp = train_data2.drop([\"horsepower\"], axis = 1)#We have seen the horsepower column has null values and we have to split for further learning\ny_imp = train_data2.horsepower #Select horsepower as label to be predicted \n\nX_train, X_test, Y_train, Y_test = train_test_split(x_imp,y_imp, test_size  = 0.3,random_state = 42)#appply split on non-null and null column\n\nlr = LinearRegression()#instantiate liner regression model\nlr.fit(X_train,Y_train)#fit the training date to our model\n\nnull_data2 = null_data.copy()#copy the null data to null_data2 for saving the original\nnull_data2 = null_data2.drop(\"horsepower\", axis = 1)##copy the horsepower data to null_data2 for saving the original\n\n#start loop iteration for each null cell and append the prediction to null cells\npredictions = []\nfor i in range(null_data2.shape[0]):\n    predictions.append(null_data2.iloc[i,:])\n\n#append all null values to the original data\nvalues = []\nfor i in range(len(predictions)):\n    for j in range(null_data2.shape[1]):\n        values.append(predictions[i][j])\n\n#start looping fora each cell with null value\n#instantaite some local variables for the loop      \ni = 0\nj = null_data2.shape[1]\nlr_predictions =[]\n\nfor a in range(0,null_data2.shape[0]):\n    print(\"Prediction {}\".format(a+1))\n    print(lr.predict((np.array([values[i:j]]))))\n    lr_predictions.append(lr.predict((np.array([values[i:j]])))[0])\n    print(\"---------------\")\n    i = i+(int(len(values) / len(predictions)))\n    j = j+(int(len(values) / len(predictions)))\n\nnull_index = train_data[train_data[\"horsepower\"].isna()].index\n\n#Append the predicted null values to our original dataset\nfor i in range(len(null_index)):\n    train_data[\"horsepower\"][null_index[i]] = lr_predictions[i]\n\n#print the number of missing values after imputation\nprint(\"Missing Values: {}\".format(train_data.isnull().sum().sum()))","metadata":{"id":"0i7C0X2UvaFm","outputId":"2a572148-0f97-4109-9715-8c58ffd225e7","execution":{"iopub.status.busy":"2021-08-13T12:51:24.831956Z","iopub.execute_input":"2021-08-13T12:51:24.832365Z","iopub.status.idle":"2021-08-13T12:51:24.869152Z","shell.execute_reply.started":"2021-08-13T12:51:24.832332Z","shell.execute_reply":"2021-08-13T12:51:24.868283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Remove Outliers","metadata":{"id":"0u-ge_Ejnl1k"}},{"cell_type":"code","source":"low = .05 #lower quntile\nhigh = .95 #Upper quantile \n\n# Step 1: compute 5% percentile and the 95% percentile of each column in the dataset\nquantile_df = train_data.quantile([low, high])\n\n# Step 2: perform outlier removal fornumerical columns\n# COMPLETE\nfeatures=['displacement', 'horsepower', 'weight', 'acceleration','fuel (L/100km)']\n          \nfor i in features:\n  train_data_rm = train_data[(train_data[i] > quantile_df.loc[0.05, i]) & (train_data[i] < quantile_df.loc[0.95, i])]\n  print('Number of rows after outlier removal: {}'.format(train_data_rm.shape[0]))\n   \ntrain_data=train_data_rm ","metadata":{"id":"I35kHR9Z_hBb","outputId":"88a7cf55-ade8-4de3-c4b3-7acfad88c142","execution":{"iopub.status.busy":"2021-08-13T12:51:24.870286Z","iopub.execute_input":"2021-08-13T12:51:24.870553Z","iopub.status.idle":"2021-08-13T12:51:24.884849Z","shell.execute_reply.started":"2021-08-13T12:51:24.870527Z","shell.execute_reply":"2021-08-13T12:51:24.884243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Assign X and Y","metadata":{"id":"kcwx2wf1MBdV"}},{"cell_type":"code","source":"# Divide the column 'x = training covariates and 'y=label as output\n\nx = train_data.drop(columns=['fuel (L/100km)'])#remove output label \n\ny = train_data['fuel (L/100km)']# assign output label for y","metadata":{"id":"83dhvRioZEWj","execution":{"iopub.status.busy":"2021-08-13T12:51:24.885755Z","iopub.execute_input":"2021-08-13T12:51:24.886121Z","iopub.status.idle":"2021-08-13T12:51:24.890972Z","shell.execute_reply.started":"2021-08-13T12:51:24.886094Z","shell.execute_reply":"2021-08-13T12:51:24.88981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# One-hot Encoding","metadata":{"id":"onKK2DTZ3Wna"}},{"cell_type":"code","source":"#Encode all catagorical columns in the training set \nx[\"origin\"] = x[\"origin\"].astype(str)\nx[\"cylinders\"] = x[\"cylinders\"].astype(str)\nx = pd.get_dummies(x)\n\n\n#Encode all catagorical columns in the testing set \ntest_data[\"origin\"] = test_data[\"origin\"].astype(str) \ntest_data[\"cylinders\"] = test_data[\"cylinders\"].astype(str)\ntest_data = pd.get_dummies(test_data)\n\n#Add cylinders with value 5 in testing set with 0 values, since there is no cylinder value with 5\n#we have to apply the code to make our training and testing columns equlal size\ntest_data.insert(loc=7, column='cylinders_5', value = 0)","metadata":{"id":"QCjAFW9YIL4D","execution":{"iopub.status.busy":"2021-08-13T12:51:24.892384Z","iopub.execute_input":"2021-08-13T12:51:24.8927Z","iopub.status.idle":"2021-08-13T12:51:24.918004Z","shell.execute_reply.started":"2021-08-13T12:51:24.892674Z","shell.execute_reply":"2021-08-13T12:51:24.917134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the dataset","metadata":{"id":"usCueVDeMHyn"}},{"cell_type":"code","source":"#Split our data for training and testing \n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=0)\n","metadata":{"id":"ivrnGs2SjlJ8","execution":{"iopub.status.busy":"2021-08-13T12:51:24.919437Z","iopub.execute_input":"2021-08-13T12:51:24.919819Z","iopub.status.idle":"2021-08-13T12:51:24.926085Z","shell.execute_reply.started":"2021-08-13T12:51:24.919778Z","shell.execute_reply":"2021-08-13T12:51:24.925132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the shapes for training and testing of our datas\nprint(\"Shape of Training data is:\",x_train.shape)\nprint(\"Shape of Testing data is:\",x_test.shape)","metadata":{"id":"0rShuiMeOCml","outputId":"ee0af3c4-6545-4531-b513-7c06fb324a31","execution":{"iopub.status.busy":"2021-08-13T12:51:24.927668Z","iopub.execute_input":"2021-08-13T12:51:24.92809Z","iopub.status.idle":"2021-08-13T12:51:24.937745Z","shell.execute_reply.started":"2021-08-13T12:51:24.928046Z","shell.execute_reply":"2021-08-13T12:51:24.936851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n---\n\n\n#MODELING\n\n---\n\n","metadata":{"id":"_UzIKzU-EAI7"}},{"cell_type":"markdown","source":"#Model 1\n># Support Vector Regression (SVR)","metadata":{"id":"-EdF-Q7kNZz0"}},{"cell_type":"code","source":"#Support Vector Machine\n\n# Step 1: Instantiate the Model for Support Vector Regressor`\nsvm_regr = make_pipeline(StandardScaler(),SVR(C=8,epsilon=2e-1))\nsvm_regr.fit(x_train,y_train)\n  \n# Step 2: Predict label on training set\ny_train_pred = svm_regr.predict(x_train)\n# Step 3: Compute RMSE on training set \nprint('RMSE on Training Data on :', np.sqrt(mean_squared_error(y_train, y_train_pred)))\n# Step 4: Predict label on test set\ny_test_pred = (svm_regr.predict(x_test))\n# Step 5: Compute RMSE on test set \nprint('RMSE on Testing Data  :', np.sqrt(mean_squared_error(y_test, y_test_pred)),'\\n')\n\n# Save your predictions to a DataFrame\nprediction = svm_regr.predict(test_data)\nmy_submission = pd.DataFrame(prediction)\nmy_submission=my_submission.rename(columns={0: \"predictions\"})\nmy_submission.to_csv('my_submission_svm.csv',index=True,index_label='id')\n#files.download('my_submission_svm.csv')\n","metadata":{"id":"JAarj8JB47Ou","outputId":"54a26ca1-5d28-4cd6-eeac-7eb7d0b2ac27","execution":{"iopub.status.busy":"2021-08-13T12:51:24.94057Z","iopub.execute_input":"2021-08-13T12:51:24.941035Z","iopub.status.idle":"2021-08-13T12:51:24.971896Z","shell.execute_reply.started":"2021-08-13T12:51:24.940994Z","shell.execute_reply":"2021-08-13T12:51:24.971138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Model 2\n># Liner Regression Model","metadata":{"id":"WuXseJCMMHMI"}},{"cell_type":"code","source":"# Step 1: Instantiate the Liner regression Model \nliner_model=LinearRegression()\nliner_model.fit(x,y)\n\n# Step 2: Predict label on training set\ny_train_pred = liner_model.predict(x_train)\n\n# Step 3: Compute RMSE on training set \nprint('RMSE on Training Data:', np.sqrt(mean_squared_error(y_train, y_train_pred)))\n\n# Step 4: Predict label on test set\ny_test_pred = liner_model.predict(x_test)\n\n# Step 5: Compute RMSE on test set \nprint('RMSE on Testing Data: ', np.sqrt(mean_squared_error(y_test, y_test_pred)))\n\n# Save your predictions to a DataFrame\nliner_pred=liner_model.predict(test_data)\nmy_submission = pd.DataFrame(liner_pred)\nmy_submission=my_submission.rename(columns={0: \"predictions\"})\nmy_submission.to_csv('my_submission_liner.csv',index=True,index_label='id')\n#files.download('my_submission_liner.csv')","metadata":{"id":"coTnHOCRZJ12","outputId":"87744893-1154-4c10-ee7a-a28ce92cc9a9","execution":{"iopub.status.busy":"2021-08-13T12:51:24.97314Z","iopub.execute_input":"2021-08-13T12:51:24.973554Z","iopub.status.idle":"2021-08-13T12:51:24.996825Z","shell.execute_reply.started":"2021-08-13T12:51:24.973516Z","shell.execute_reply":"2021-08-13T12:51:24.995681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Model 3\n> # Random Forest","metadata":{"id":"jF82aKVOE5Bm"}},{"cell_type":"code","source":"# Step 1: Instantiate the Random Forest Model \nx, y = make_regression(n_features=8, n_informative=3,random_state=0, shuffle=False)\nrand_regr = RandomForestRegressor(max_depth=3, random_state=0)\nrand_regr.fit(x_train,y_train)\n\n# Step 2: Predict label on training set\ny_train_pred = rand_regr.predict(x_train)\n# Step 3: Compute RMSE on training set \nprint('RMSE on Training Data:', np.sqrt(mean_squared_error(y_train, y_train_pred)))\n\n# Step 4: Predict label on test set\ny_test_pred = (rand_regr.predict(x_test))\n# Step 5: Compute RMSE on test set \nprint('RMSE on Testing Data: ', np.sqrt(mean_squared_error(y_test, y_test_pred)))\n\n# Save your predictions to a DataFrame\nmy_submission = pd.DataFrame(rand_regr.predict(test_data))\nmy_submission=my_submission.rename(columns={0: \"predictions\"})\nmy_submission.to_csv('my_submission_random.csv',index=True,index_label='id')\n#files.download('my_submission_random.csv')","metadata":{"id":"FyAEXEWRCRQx","outputId":"02bbd740-cff8-4150-91fd-cae208baab7d","execution":{"iopub.status.busy":"2021-08-13T12:51:24.997874Z","iopub.execute_input":"2021-08-13T12:51:24.998154Z","iopub.status.idle":"2021-08-13T12:51:25.221969Z","shell.execute_reply.started":"2021-08-13T12:51:24.998126Z","shell.execute_reply":"2021-08-13T12:51:25.221003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 4\n# Decision Tree Model\n","metadata":{"id":"QFA4S8OlFCb3"}},{"cell_type":"code","source":"# Step1: Instantiate Decision Tree Model\n\ndec_regr = DecisionTreeRegressor(max_depth=6)\ndec_regr.fit(x_train,y_train)\n\n# Step 2: Predict label on training set\ny_tran_pred = dec_regr.predict(x_train)\n# Step 3: Compute RMSE on training set \nprint('RMSE on Training Data:', np.sqrt(mean_squared_error(y_train, y_train_pred)))\n# Step 4: Predict label on test set\ny_test_pred = dec_regr.predict(x_test)\n# Step 5: Compute RMSE on test set \nprint('RMSE on Testing Data: ', np.sqrt(mean_squared_error(y_test, y_test_pred)))\n\n# Save your predictions to a DataFrame\nmy_submission = pd.DataFrame(dec_regr.predict(test_data))\nmy_submission=my_submission.rename(columns={0: \"predictions\"})\nmy_submission.to_csv('my_submission_decison.csv',index=True,index_label='id')\n#files.download('my_submission_decison.csv')","metadata":{"id":"7OlAYngd_bgD","outputId":"ff7f2fa9-3880-421f-9101-9f319f6e3ec0","execution":{"iopub.status.busy":"2021-08-13T12:51:25.223323Z","iopub.execute_input":"2021-08-13T12:51:25.223626Z","iopub.status.idle":"2021-08-13T12:51:25.246727Z","shell.execute_reply.started":"2021-08-13T12:51:25.223597Z","shell.execute_reply":"2021-08-13T12:51:25.245963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Once you have generated predictions on the holdout and test data, create a submission file\nYour submission file should follow the Kaggle submission template.\n","metadata":{"id":"OwpYLDdVZK24"}},{"cell_type":"code","source":"# Here's a sample submission file\nsample_submission = pd.read_csv('https://raw.githubusercontent.com/onefishy/Rwanda-course-2020/master/Competition_data/sampleSubmission.csv') # read in the data as a DataFrame\nsample_submission.head() # show the first 3 rows of the dataset","metadata":{"id":"VQQqCw-8OB4k","outputId":"2cd7a416-eb6a-4973-9b7f-76b49adb14cb","execution":{"iopub.status.busy":"2021-08-13T12:51:25.248043Z","iopub.execute_input":"2021-08-13T12:51:25.248447Z","iopub.status.idle":"2021-08-13T12:51:25.337773Z","shell.execute_reply.started":"2021-08-13T12:51:25.248407Z","shell.execute_reply":"2021-08-13T12:51:25.337084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n\n## From all the given Models we have selected Support Vector Regression for final result\n\n---\n\n","metadata":{"id":"k6t5f2SAFGtW"}},{"cell_type":"code","source":"# Save your predictions to a DataFrame\nmy_submission = pd.DataFrame(svm_regr.predict(test_data))\nmy_submission=my_submission.rename(columns={0: \"predictions\"})\nmy_submission.to_csv('my_submission.csv',index=True,index_label='id')","metadata":{"id":"k2qs_t4XPXJg","execution":{"iopub.status.busy":"2021-08-13T12:51:25.338692Z","iopub.execute_input":"2021-08-13T12:51:25.338947Z","iopub.status.idle":"2021-08-13T12:51:25.349125Z","shell.execute_reply.started":"2021-08-13T12:51:25.338922Z","shell.execute_reply":"2021-08-13T12:51:25.348145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check that your submission looks the same as the samples submission\npd.read_csv('my_submission.csv')","metadata":{"id":"-F56KA2sPs7y","outputId":"bd43af0d-1912-4f52-deea-201a36303709","execution":{"iopub.status.busy":"2021-08-13T12:51:25.350468Z","iopub.execute_input":"2021-08-13T12:51:25.350872Z","iopub.status.idle":"2021-08-13T12:51:25.368899Z","shell.execute_reply.started":"2021-08-13T12:51:25.350832Z","shell.execute_reply":"2021-08-13T12:51:25.367893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save and download the submission file and upload to the Kaggle website\n\nThen download `my_submission.csv` by running the following line and submit to the [Kaggle](https://www.kaggle.com/t/b9bc778c9e8842d28c5526f578e6c348) compeition website.","metadata":{"id":"USxO2cDAPHN-"}},{"cell_type":"code","source":"#files.download('my_submission.csv')","metadata":{"id":"uCdnee70PFcX","outputId":"851eef19-7e7a-40e5-870a-bf17a4a9ed5a","execution":{"iopub.status.busy":"2021-08-13T12:51:25.369872Z","iopub.execute_input":"2021-08-13T12:51:25.370139Z","iopub.status.idle":"2021-08-13T12:51:25.399035Z","shell.execute_reply.started":"2021-08-13T12:51:25.370113Z","shell.execute_reply":"2021-08-13T12:51:25.397816Z"},"trusted":true},"execution_count":null,"outputs":[]}]}