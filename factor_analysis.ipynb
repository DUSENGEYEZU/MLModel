{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DUSENGEYEZU_LONGIN_13_factor_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDMHeGcjNraT"
      },
      "source": [
        "# DAY 13: Nonnegative Matrix Factorization\n",
        "\n",
        "\n",
        "### Machine Learning and Computational Statistics (DSC6232)\n",
        "\n",
        "#### Instructors: Weiwei Pan, Melanie Fernandez, Pavlos Protopapas\n",
        "\n",
        "#### Due: August 12th, 2:00 pm Kigali Time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reQ30R54OdJ7"
      },
      "source": [
        "**First name**: LONGIN\n",
        "\n",
        "\n",
        "**Last name**: DUSENGEYEZU\n",
        "\n",
        "**ID**:220020609"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpRW0Pz0OdoU"
      },
      "source": [
        "## Learning Goals:\n",
        "\n",
        "1. learn how to process and encode text data\n",
        "2. understand how to analyze documents using a simple topic model \n",
        "3. learn how to interpret nonnegative matrix factorization models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXr0ChYk3Suj"
      },
      "source": [
        "### Load necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQjmfrXs3PZ-"
      },
      "source": [
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from IPython.display import display\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELmGEE5p3XtF"
      },
      "source": [
        "\n",
        "### We include auxiliary functions here that we will need to use later  **No need to read in details!**\n",
        "\n",
        "We include auxiliary functions here that we will need to use later\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJRD0WyL3Uwz"
      },
      "source": [
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Topic #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "    print()\n",
        "\n",
        "def print_features(feature_names, num_columns=5):\n",
        "    padding = num_columns - len(feature_names) % num_columns\n",
        "    feature_names += [''] * (padding * (padding != num_columns))\n",
        "    feature_names = np.array(feature_names).reshape(-1, num_columns)\n",
        "    display(pd.DataFrame(feature_names, columns=[''] * num_columns).reset_index(drop=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DagRffRp9ADz"
      },
      "source": [
        "# Topic Modeling for News Articles\n",
        "\n",
        "This exercise is designed to help you transform and model textual data. You may find the tutorial [here](http://scikit-learn.org/stable/modules/feature_extraction.html) helpful.\n",
        "\n",
        "You will encode a small set of news articles (i.e. represent them as count vectors) and model this set using a Nonnegative Factorization Model. Your goal is to discover a latent set of topic underlying the articles and discover which topics appear in each article.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugiBG-cL3cFi"
      },
      "source": [
        "### Load-in the data and examine it\n",
        "\n",
        "We use the `fetch_20newsgroups` function from `sklearn` to load a set of articles in the categories: \"medicine\", \"religion\" and \"motorcycles\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIhnx8QK3Zmf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447e1e04-e2e4-4d9f-b4c1-8f9ada46e969"
      },
      "source": [
        "# Load the data set\n",
        "print(\"Loading dataset...\")\n",
        "data, _ = fetch_20newsgroups(shuffle=False, remove=('headers', 'footers', 'quotes'),\n",
        "                             return_X_y=True, categories=['sci.med', 'soc.religion.christian', 'rec.motorcycles'])\n",
        "print('Done.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gB97T9d-wz7"
      },
      "source": [
        "We check to see how many articles we have loaded. We also print two articles form this set to see what they look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f2Ujdei3gtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4229a68f-8d1e-4b60-f7f5-b90c5b3e905d"
      },
      "source": [
        "# Print the number of articles in the data\n",
        "print('Number of data points: {}\\n'.format(len(data)))\n",
        "\n",
        "# Print out an example article from the data\n",
        "print('Example articles:\\n\\n')\n",
        "print('*' * 10 + ' Example 1 ' + '*' * 10)\n",
        "print(data[0])\n",
        "\n",
        "# Print out another article from the data\n",
        "print('\\n\\n' + '*' * 10 + ' Example 2 ' + '*' * 10)\n",
        "print(data[5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of data points: 1791\n",
            "\n",
            "Example articles:\n",
            "\n",
            "\n",
            "********** Example 1 **********\n",
            "Does anyone on this newsgroup happen to know WHY morphine was\n",
            "first isolated from opium?  If you know why, or have an idea for where I\n",
            "could look to find this info, please mail me.\n",
            "\tCSH\n",
            "any suggestionas would be greatly appreciated\n",
            "\n",
            "--\n",
            " \"Kilimanjaro is a pretty tricky climb. Most of it's up, until you reach\n",
            "the very, very top, and then it tends to slope away rather sharply.\"\n",
            "\t\t\t\t\tSir George Head, OBE (JC)\n",
            "\n",
            "\n",
            "********** Example 2 **********\n",
            "I just noticed that my halogen table lamp runs off 12 Volts.\n",
            "The big thinngy that plugs into the wall says 12 Volts DC,  20mA\n",
            "\n",
            "The question is: Can I trickle charge the battery on my CB650\n",
            "with it?\n",
            "\n",
            "I don't know the rating of the battery, but it is a factory\n",
            "intalled one. \n",
            "\n",
            "\n",
            "Thanks,\n",
            "Sanjay\n",
            "\n",
            "-- \n",
            "   '81 CB650 \t\t\t\t\t\tDoD #1224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzOpUB-cT7GR"
      },
      "source": [
        "### Encode the data as count vectors\n",
        "\n",
        "We are going to use `sklearn`'s `TfidfVectorizer` function to remove punctuations and non-meaningful words from the documents and then convert them into count vectors.\n",
        "\n",
        "**Exercise 1:** Read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for `TfidfVectorizer`, and experiment with different values for the parameters `max_df`, `min_df`, `max_features`. What do each of these parameters mean? How does changing these parameters change the count vector representation of the data?\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "**TfindVectorizer: Convert a collection of raw documents to a matrix of TF-IDF features.**\n",
        "\n",
        "**1) max_df : stands for maximum document frequency**.When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
        "\n",
        "\n",
        "Here if frequence of word in a document exceeds threshold specified now is going to be removed in the list of words that could help us to identify to pic, because may be is the word that is frequently used but it has not meaning that could help us to identify natural of topic.\n",
        "\n",
        "**2)min_df: stands for minimum  document frequency:** When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float in range of [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
        "\n",
        "Here if frequence of word in a document is below  threshold specified now is going to be removed in the list of words that could help us to identify to pic, because maybe is the word that is not  frequently used ,it has not have enough information   that could help us to identify natural of topic.\n",
        "\n",
        "**2)max_features:**\n",
        "If not None, build a vocabulary that only considers the top max_features ordered by term frequency across the corpus.\n",
        "This really help us to set all words in matrix form just to represent all words that form each topic \n",
        "\n",
        "\n",
        "\n",
        "**NB: from this experiment and from the definition of each parameter we have seen that if we increase max_df, the number of words in matrix or per topic is increased but decrease if max_df is reduced. From min_df is increased number of word is reduced but contrally if increased**  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XEBy0iz3jgp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "outputId": "d67cb5cb-cb12-44a9-d04c-d4bec9fe1991"
      },
      "source": [
        "# Step 1: Reduce the size of the data\n",
        "n_samples = 1000\n",
        "data_samples = random.sample(data, min(n_samples, len(data)))\n",
        "\n",
        "# Step 2: Choose the number of features, or important words, to extract\n",
        "n_features = 1000\n",
        "\n",
        "# Step 3: Extract tf-idf features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')\n",
        "\n",
        "# Step 4: Encode the documents as normalized count vectors\n",
        "vectorized_data = tfidf_vectorizer.fit_transform(data_samples)\n",
        "\n",
        "# Step 5: Get learned feature names\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "# Step 6: Select a samplee of the learned features \n",
        "sample_of_features = random.sample(tfidf_feature_names, 100)\n",
        "\n",
        "# Step 7: Print that sample of learned feature names\n",
        "print_features(sample_of_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>send</td>\n",
              "      <td>woman</td>\n",
              "      <td>following</td>\n",
              "      <td>nature</td>\n",
              "      <td>animals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17</td>\n",
              "      <td>newsletter</td>\n",
              "      <td>deal</td>\n",
              "      <td>able</td>\n",
              "      <td>mean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>letter</td>\n",
              "      <td>specific</td>\n",
              "      <td>completely</td>\n",
              "      <td>disciples</td>\n",
              "      <td>claims</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>news</td>\n",
              "      <td>national</td>\n",
              "      <td>forget</td>\n",
              "      <td>methods</td>\n",
              "      <td>treated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>certainty</td>\n",
              "      <td>medical</td>\n",
              "      <td>exactly</td>\n",
              "      <td>realize</td>\n",
              "      <td>prayers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>road</td>\n",
              "      <td>pitt</td>\n",
              "      <td>sins</td>\n",
              "      <td>effects</td>\n",
              "      <td>universe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>outside</td>\n",
              "      <td>directly</td>\n",
              "      <td>major</td>\n",
              "      <td>friend</td>\n",
              "      <td>divine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>effect</td>\n",
              "      <td>changed</td>\n",
              "      <td>wish</td>\n",
              "      <td>period</td>\n",
              "      <td>posting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>doesn</td>\n",
              "      <td>power</td>\n",
              "      <td>modern</td>\n",
              "      <td>19</td>\n",
              "      <td>knowledge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>spiritual</td>\n",
              "      <td>trouble</td>\n",
              "      <td>jewish</td>\n",
              "      <td>careful</td>\n",
              "      <td>say</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>light</td>\n",
              "      <td>aside</td>\n",
              "      <td>wasn</td>\n",
              "      <td>religious</td>\n",
              "      <td>husband</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>good</td>\n",
              "      <td>tire</td>\n",
              "      <td>happened</td>\n",
              "      <td>best</td>\n",
              "      <td>clearly</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>recommend</td>\n",
              "      <td>peter</td>\n",
              "      <td>wrote</td>\n",
              "      <td>concerned</td>\n",
              "      <td>force</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>glutamate</td>\n",
              "      <td>ca</td>\n",
              "      <td>sort</td>\n",
              "      <td>needles</td>\n",
              "      <td>thinking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>attention</td>\n",
              "      <td>personal</td>\n",
              "      <td>issue</td>\n",
              "      <td>truth</td>\n",
              "      <td>required</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>ones</td>\n",
              "      <td>kill</td>\n",
              "      <td>prayer</td>\n",
              "      <td>capable</td>\n",
              "      <td>including</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>add</td>\n",
              "      <td>trying</td>\n",
              "      <td>black</td>\n",
              "      <td>creed</td>\n",
              "      <td>exist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>son</td>\n",
              "      <td>uk</td>\n",
              "      <td>intellect</td>\n",
              "      <td>paper</td>\n",
              "      <td>hit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>member</td>\n",
              "      <td>complete</td>\n",
              "      <td>heaven</td>\n",
              "      <td>statements</td>\n",
              "      <td>time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>clinical</td>\n",
              "      <td>fun</td>\n",
              "      <td>common</td>\n",
              "      <td>gordon</td>\n",
              "      <td>price</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                            \n",
              "0        send       woman   following      nature    animals\n",
              "1          17  newsletter        deal        able       mean\n",
              "2      letter    specific  completely   disciples     claims\n",
              "3        news    national      forget     methods    treated\n",
              "4   certainty     medical     exactly     realize    prayers\n",
              "5        road        pitt        sins     effects   universe\n",
              "6     outside    directly       major      friend     divine\n",
              "7      effect     changed        wish      period    posting\n",
              "8       doesn       power      modern          19  knowledge\n",
              "9   spiritual     trouble      jewish     careful        say\n",
              "10      light       aside        wasn   religious    husband\n",
              "11       good        tire    happened        best    clearly\n",
              "12  recommend       peter       wrote   concerned      force\n",
              "13  glutamate          ca        sort     needles   thinking\n",
              "14  attention    personal       issue       truth   required\n",
              "15       ones        kill      prayer     capable  including\n",
              "16        add      trying       black       creed      exist\n",
              "17        son          uk   intellect       paper        hit\n",
              "18     member    complete      heaven  statements       time\n",
              "19   clinical         fun      common      gordon      price"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_mniF8UV7_T"
      },
      "source": [
        "**Exercise 2:** Print the normalized count vector representation of a single document. What kind of numbers are in this vector? What do these numbers represent? ***Hint:*** recall how we process count vectors before fitting a nonnegative matrix factorization model. \n",
        "\n",
        "**Answer**:\n",
        "\n",
        "The numbers on the left are one-hot encoded identifiers of the different words. On the right, these numbers are frequencies of occurrence of specific numbers in the documents\n",
        "\n",
        "The `TfidfVectorizer` function normalizes the count vectors, what does this mean and why is this step necessary?\n",
        "\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "This means that the function expresses the word counts as percentage frequencies. This step is necessary in extracting the most important features or words in the documents. Also this percentage is needed in needed in matrix represtation even in NMF model in dowun word in this notebook "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WeIyRJoVfVI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f991ccd8-082c-4c17-fa76-528c5b651229"
      },
      "source": [
        "# Step 1: Print the normalized count vector representation of a single document\n",
        "n = 10\n",
        "print('The normalized ount vector representation of the {}-th document'.format(n))\n",
        "print(vectorized_data[n])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The normalized ount vector representation of the 10-th document\n",
            "  (0, 696)\t0.3536207863458619\n",
            "  (0, 611)\t0.507835027091325\n",
            "  (0, 961)\t0.3811902989395354\n",
            "  (0, 263)\t0.3307424291717216\n",
            "  (0, 641)\t0.3307424291717216\n",
            "  (0, 519)\t0.24510009791277151\n",
            "  (0, 806)\t0.27453259784821016\n",
            "  (0, 826)\t0.3021021104418836\n",
            "  (0, 513)\t0.16205209470190518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ_7F2kpWoLr"
      },
      "source": [
        "### Fit a Nonnegative Matrix Factorization Model to the data\n",
        "\n",
        "Now that our data has been encoded as normalized count vectors, we can fit an NMF model to it.\n",
        "\n",
        "**Exercise 3:** Fit an NMF model with 10 topics, print out the top words associated to each topic. Can you interpret what each topic is about?\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "Yes. I can interpret what most topics are about. For example, topic 2 mentions biblical names and words such as Jesus and sin, signifying christian preaching(religion). Topic 3 is a scientific theory and process(motorcycles).Topic 4 is about food and nutrition(medicine). Topic 5 is about health and medicine(medicine). Topic 6 is about catholic church beliefs(religion). Topic 8 is on motorcycles and their insurance(motorcycles)\n",
        "\n",
        "NB: even if we can identify same but other we can not and i think this is not good number of topic we can choose.\n",
        "\n",
        "Fit an NMF model with 2 topics, print out the top words associated to each topic. Can you interpret what each topic is about?\n",
        "\n",
        "**Answer**:\n",
        "\n",
        " \n",
        "It is hard to interpret what each topic is about when we are limited to 2 topics. and it became completly worse compered to the result where we have used 10 topics.\n",
        "\n",
        "\n",
        "Find an appropriate number of topics. Why is this number appropriate?\n",
        "\n",
        "**Answer**:\n",
        "The appropriate number of topics would be between 2 and 10. This is because, it ensures that unique words are spread across different topics. This is the factor that makes unique topic identification from the bag of words possible  also alleast you can identify what each topic is all about. but also not 100%.\n",
        "Here it is better to use same technical method of choosing hyperparameters like cross-validation  or others.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VUYYgtt3lKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38df15c2-0b0b-4415-b3d6-fe70a7c76eb2"
      },
      "source": [
        "# Step 1: Fit NMF model\n",
        "nmf = NMF(n_components=8, alpha=0.1, l1_ratio=0.5).fit(vectorized_data)\n",
        "\n",
        "# Step 2: Print out the learned topics\n",
        "print('Topics learned by the NMF:')\n",
        "print_top_words(nmf, tfidf_feature_names, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topics learned by the NMF:\n",
            "Topic #0: don know like just people think time does good ve\n",
            "Topic #1: cadre shameful dsl geb chastity n3jxp surrender pitt intellect banks\n",
            "Topic #2: bike helmet bikes ride riding miles honda dod used buy\n",
            "Topic #3: msg food chinese glutamate eat foods taste people use reaction\n",
            "Topic #4: god faith bible lord christ hell truth life heaven christians\n",
            "Topic #5: thanks mail know reference new program hi thing responses different\n",
            "Topic #6: jesus father christ john son apostles sin come did kingdom\n",
            "Topic #7: dog dogs road street got saw face running left lane\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9wU5R5nXdRq"
      },
      "source": [
        "**Exercise 4:** Pick a document and print out the combinations of topics in that document. Which combinations of topics are contained in this article? Do you agree with the the combination of topics learned by the model?\n",
        "\n",
        "**Answer**\n",
        "\n",
        "\n",
        "The document combines the topic of health and motorcycles. I agree with the combination of topics since they both add up to the common theme of healthy living through both eating well and exercising sufficiently  and also even if you can say 100% if about healthy diet and motorcycles but it has higher probability to be those topics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZeLGjPMDjiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d974c9-d7ca-49cb-8015-e4c80b2cd810"
      },
      "source": [
        "# Step 1: get the document to topic matrix each row of this matrix is the combination of topics in the document\n",
        "document_to_topic = nmf.transform(vectorized_data)\n",
        "\n",
        "# Step 2: print the shape of this matrix to verify that we have 1000 documents and 10 topic\n",
        "print(document_to_topic.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF81swCccOOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12eadb32-bac6-4ecf-8f67-44d7addeffab"
      },
      "source": [
        "# Step 3: print an article and the predicted combination of topics in this article\n",
        "n = 10\n",
        "\n",
        "# Print the predicted combination of topics in this article\n",
        "print('\\n\\n' + '*' * 10 + ' Predicted combination of topics ' + '*' * 10 + '\\n\\n')\n",
        "print(document_to_topic[n])\n",
        "\n",
        "# Print out an example article from the data\n",
        "print('\\n\\n' + '*' * 10 + ' Article {} '.format(n) + '*' * 10)\n",
        "print(data[n])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "********** Predicted combination of topics **********\n",
            "\n",
            "\n",
            "[0.03705662 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n",
            "\n",
            "\n",
            "********** Article 10 **********\n",
            "\n",
            "I attribute my success to several factors:\n",
            "\n",
            "Very low fat.  Except when someone else has cooked a meal for me,\n",
            "I only eat fruit, vegetables, and whole grain or bran cereals.  I\n",
            "estimate I only get about 5 to 10 percent of my calories from fat.\n",
            "\n",
            "Very little sugar or salt.\n",
            "\n",
            "Very high fiber.  Most Americans get about 10 grams.  25 to 35 are\n",
            "recommended.  I get between 50 and 150.  Sometimes 200.  (I've heard\n",
            "of people taking fiber pills.  It seems unlikely that pills can\n",
            "contain enough fiber to make a difference.  It would be about as\n",
            "likely as someone getting fat by popping fat pills.  Tablets are\n",
            "just too small, unless you snarf down hundreds of them daily.)\n",
            "\n",
            "My \"clean your plate\" conditioning works *for* me.  Eating the last\n",
            "10% takes half my eating time, and gives satiety a chance to catch\n",
            "up, so I don't still feel hungry and go start eating something else.\n",
            "\n",
            "I don't eat when I'm not hungry (unless I'm sure I'll get hungry\n",
            "shortly, and eating won't be practical then).\n",
            "\n",
            "I bike to work, 22 miles a day, year round.  Fast.  I also bike to\n",
            "stores, movies, and everywhere else, as I've never owned a car.\n",
            "I estimate this burns about 1000 calories a day.  It also helps\n",
            "build and maintain muscle mass, prevent insulin resistance (diabetes\n",
            "runs in my family), and increase my metabolism.  (Even so, my\n",
            "metabolism is so low that when I'm at rest I'm most comfortable\n",
            "with a temperature in the 90s (F), and usually wear a sweater if\n",
            "it drops to 80.)  Cycling also motivates me to avoid every excess\n",
            "ounce.  (Cyclists routinely pay a premium for cycling products that\n",
            "weigh slightly less than others.  But it's easier and cheaper to trim\n",
            "weight from the rider than from the vehicle.)\n",
            "\n",
            "There's no question in my mind that my metabolism is radically\n",
            "different from that of most people who have never been fat.  Fortunately,\n",
            "it isn't different in a way that precludes excellent health.\n",
            "\n",
            "Obviously, I can't swear that every obese person who does what I've\n",
            "done will have the success I did.  But I've never yet heard of one who\n",
            "did try it and didn't succeed.\n",
            "\n",
            "\n",
            "I'm sure everyone's weight cycles, whether or not they've ever been fat.\n",
            "I usually eat extremely little salt.  When I do eat something salty,\n",
            "my weight can increase overnight by as much as ten pounds.  It comes\n",
            "off again over a week or two.\n",
            "-- \n",
            "Keith Lynch, kfl@access.digex.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R9CgwwiDXVX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}